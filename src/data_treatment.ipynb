{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4fca0eae666ee8a363f6e42c2ce25cbdba496d9098d60dd72f409fa19626efc9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "#import logging\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.model_selection import train_test_split\n",
    "# keras.preprocessing.sequence import pad_sequences\n",
    "#from torch.utilis.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", delimiter=\",\", header=0, names=[\"id\", \"keyword\", \"location\", \"text\",\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dimensions of training data: (7613, 5)\n"
    }
   ],
   "source": [
    "print(\"dimensions of training data:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the number of sentences seen as disaster telling: 3271\n"
    }
   ],
   "source": [
    "print(\"the number of sentences seen as disaster telling:\", df[df.target==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the number of observations of keyword == naN is: 61\n"
    }
   ],
   "source": [
    "print(\"the number of observations of keyword == naN is:\",df[pd.isna(df.keyword)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   text\n6873  80 @UChicago faculty members pushing universit...\n5094  Fukushima Nuclear Disaster | Increased Thyroid...\n2812  DISASTER AVERTED: Police kill gunman with 'hoa...\n7305  @Jennife29916207 I was thinking about you toda...\n2882  We happily support mydrought  a project bringi...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>6873</td>\n      <td>80 @UChicago faculty members pushing universit...</td>\n    </tr>\n    <tr>\n      <td>5094</td>\n      <td>Fukushima Nuclear Disaster | Increased Thyroid...</td>\n    </tr>\n    <tr>\n      <td>2812</td>\n      <td>DISASTER AVERTED: Police kill gunman with 'hoa...</td>\n    </tr>\n    <tr>\n      <td>7305</td>\n      <td>@Jennife29916207 I was thinking about you toda...</td>\n    </tr>\n    <tr>\n      <td>2882</td>\n      <td>We happily support mydrought  a project bringi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "df[df.target == 1].sample(5)[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.text.values\n",
    "targets = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 488kB/s]\n"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "@mickinyman @TheAtlantic That or they might be killed in an airplane accident in the night a car wreck! Politics at it's best.\n['@', 'mick', '##iny', '##man', '@', 'the', '##at', '##lan', '##tic', 'that', 'or', 'they', 'might', 'be', 'killed', 'in', 'an', 'airplane', 'accident', 'in', 'the', 'night', 'a', 'car', 'wreck', '!', 'politics', 'at', 'it', \"'\", 's', 'best', '.']\n"
    }
   ],
   "source": [
    "print(texts[150])\n",
    "print(tokenizer.tokenize(texts[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "max length to set to is 84\n"
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for text in texts:\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len,len(input_ids))\n",
    "print(\"max length to set to is\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, max_length=84, truncation=True, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids,dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "targets = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, targets)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset)-train_size\n",
    "\n",
    "random.seed(10)\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size= batch_size)\n",
    "validation_data_loader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 433/433 [00:00<00:00, 170kB/s]\nDownloading: 100%|██████████| 440M/440M [05:50<00:00, 1.26MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_data_loader)*epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy (preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)/len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n================ Epoch 1 / 4 ================\nTraining ongoing...\n"
    }
   ],
   "source": [
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "#Storing training stats\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print(\"================ Epoch {:} / {:} ================\".format(epoch_i + 1, epochs))\n",
    "    print('Training ongoing...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    #reseting total loss for current epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #Activating train mode\n",
    "    model.train\n",
    "\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "        \n",
    "        # Progress visualization every 15 batches\n",
    "        if step % 15 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time - t0)\n",
    "            print(' Batch {: >5,} of {:>,}.  Elapsed: {:}.'.format(step, len(train_data_loader), elapsed))\n",
    "        \n",
    "        # Unpacking batches\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        #Clear Grads\n",
    "        model.zero_grad()\n",
    "\n",
    "        #performing forward pass\n",
    "        loss, loggits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels= b_labels)\n",
    "\n",
    "        #Accumulating training loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Performing Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Setting the norm of gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "\n",
    "        #Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #Updating the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "avg_train_loss = total_train_loss / len(train_data_loader)\n",
    "training_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"\")\n",
    "print(\" Avg training loss: {0:.2f}\".format(avg_train_loss))\n",
    "print(\" Training epoch took: {:}\".format(training_time)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "215\n"
    }
   ],
   "source": [
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "215\n"
    }
   ],
   "source": [
    "l0 = 0\n",
    "for i,j in enumerate(train_data_loader):\n",
    "    l0+=1\n",
    "print(l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}